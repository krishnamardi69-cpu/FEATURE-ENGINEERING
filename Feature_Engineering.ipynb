{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature Engineering**\n",
        "# ASSIGNMENT SOLUTION\n",
        "Q1- What is a parameter?\n",
        "\n",
        "A1- A parameter is a value or input that is passed to a function, method, or process to customize how it behaves.\n",
        "\n",
        "Q2- What is correlation? What does negative correlation mean?\n",
        "\n",
        "A2-Correlation is a statistical measure that shows how two variables are related — specifically, how one variable changes when the other changes.\n",
        "\n",
        "If both increase together, that’s positive correlation.\n",
        "\n",
        "If one increases while the other decreases, that’s negative correlation.\n",
        "\n",
        "If there’s no pattern, then there’s no correlation.\n",
        "\n",
        "A negative correlation means that as one variable increases, the other decreases.\n",
        "\n",
        "Example: The more you exercise, the less you weigh (generally). → Exercise ↑ → Weight ↓ → That’s a negative correlation.\n",
        "\n",
        "Q3-Define Machine Learning. What are the main components in Machine Learning?\n",
        "\n",
        "A3- Elements of Machine Learning. (Basic framework for designing ...Machine learning is a subset of artificial intelligence that enables computer systems to learn from data without being explicitly programmed. This learning process involves using algorithms to identify patterns in data, allowing systems to make predictions or decisions without human intervention. The main components of machine learning include: Data: Machine learning models learn from data, which can be structured (tables, spreadsheets) or unstructured (text, images, audio). Algorithms: These are the sets of rules and statistical techniques used to analyze and interpret data, enabling the system to learn from it. Models: These are the learned representations of the data, often in the form of mathematical equations or functions, that capture the patterns identified by the algorithms. Predictions: Machine learning models are used to make predictions or decisions based on the learned patterns.\n",
        "\n",
        "Q4- How does loss value help in determining whether the model is good or not?\n",
        "\n",
        "A4- ML | Common Loss Functions | GeeksforGeeksA model's loss value indicates how well it's learning and making predictions. A low loss value means the model is making fewer errors and is, therefore, performing well, while a high loss value suggests the model is struggling with the data and needs more refinement, according to IBM.\n",
        "\n",
        "Q5- What are continuous and categorical variables?\n",
        "\n",
        "A5- In statistics, continuous variables represent numerical values that can take any value within a specified range, while categorical variables represent non-numerical data grouped into categories or groups\n",
        "\n",
        "Q6- How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "\n",
        "A6- Categorical variables, which represent qualitative data with distinct categories, require special handling in machine learning due to their non-numerical nature. Several techniques are used to convert them into numerical representations that algorithms can process. These include one-hot encoding, label encoding, ordinal encoding, and frequency encoding. Here's a more detailed look at these techniques:\n",
        "\n",
        "One-Hot Encoding: Purpose: Converts each category into a binary column (0 or 1), indicating its presence or absence. Example: If a \"color\" feature has categories \"red\", \"blue\", and \"green\", one-hot encoding would create three new columns: \"red\", \"blue\", and \"green\". For an instance with \"blue\" as the color, the \"blue\" column would be 1, and the others would be 0. Suitable for: Nominal (unordered) categorical variables. Python Libraries: scikit-learn, pandas (using get_dummies).\n",
        "Label Encoding: Purpose: Assigns each category a unique integer value, effectively creating a numerical representation. Example: The \"color\" categories \"red\", \"blue\", \"green\" could be labeled as 0, 1, and 2, respectively. Suitable for: Scenarios where the order of categories is not important, but the categories need to be represented numerically. Python Library: scikit-learn.\n",
        "Ordinal Encoding: Purpose: Preserves the order or ranking of categories when converting them to numerical representations. Example: If a \"size\" feature has categories \"small\", \"medium\", and \"large\", ordinal encoding could assign 1, 2, and 3, respectively. Suitable for: Categorical variables with inherent order or hierarchy. Python Library: scikit-learn.\n",
        "Frequency Encoding: Purpose: Replaces each category with the count or proportion of its occurrences in the dataset. Example: If \"red\" appears 10 times, \"blue\" appears 5 times, and \"green\" appears 2 times, the frequency encoding could replace them with 10, 5, and 2, respectively. Suitable for: Categorical variables with high cardinality (many unique categories). Other Techniques: Target Encoding: Uses the mean or median of the target variable for each category as its numerical representation. Binary Encoding: Similar to one-hot encoding but uses fewer binary columns for each category, making it more suitable for high cardinality datasets\n",
        "Q7- What do you mean by training and testing a dataset?\n",
        "\n",
        "A7- In machine learning, training a dataset means using a portion of the data to teach a model how to make predictions or decisions, while testing a dataset means evaluating how well the trained model performs on new, unseen data.\n",
        "\n",
        "Q8- What is sklearn.preprocessing?\n",
        "\n",
        "A8- sklearn.preprocessing is a module in the Scikit-learn library (also called sklearn) used for preparing or transforming data before training a machine learning model.\n",
        "\n",
        "Q9- What is a Test set?\n",
        "\n",
        "A9- A test set is a portion of the dataset that is not used during training. Instead, it is used after the model is trained to evaluate how well the model performs on new, unseen data.\n",
        "\n",
        "Q10- How do we split data for model fitting (training and testing) in Python? How do you approach a Machine Learning problem?\n",
        "\n",
        "A10- In Python, data for model training and testing is typically split using train_test_split from scikit-learn. This function randomly divides the dataset into training and testing subsets, allowing for model evaluation on unseen data. Splitting Data in Python\n",
        "\n",
        "Import Necessary Libraries: Begin by importing train_test_split from sklearn.model_selection and numpy for array manipulation.\n",
        "\n",
        "Prepare Your Data: Ensure your data is in a structured format, such as a NumPy array or Pandas DataFrame.\n",
        "\n",
        "Separate Features and Labels: Identify the input features (X) and the target variable (y).\n",
        "\n",
        "Use train_test_split(): Call the train_test_split() function with your features (X) and labels (y), specifying the test_size parameter to control the proportion of data allocated to the test set (e.g., test_size=0.2 for 20% test data).\n",
        "\n",
        "Optional Parameters: You can also use the random_state parameter for reproducibility and shuffle (default is True) to ensure randomization before splitting.\n",
        "\n",
        "Split Data: The function will return four arrays: X_train, X_test, y_train, and y_test, representing the training features, testing features, training labels, and testing labels, respectively.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KqXJ5Rwq7E1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Sample data\n",
        "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
        "y = np.array([0, 1, 0, 1])\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# X_train, X_test, y_train, y_test are now ready for training and testing\n",
        ""
      ],
      "metadata": {
        "id": "tIcuF7pw8GWF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q11- Why do we have to perform EDA before fitting a model to the data?\n",
        "\n",
        "A11-EDA (Exploratory Data Analysis) is the process of understanding your data before using it to train a model. It helps you identify patterns, spot problems, and make smarter modeling choices.\n",
        "\n",
        "Q12- What is correlation?\n",
        "\n",
        "A12-Correlation is a statistical measure that describes the relationship between two variables — specifically, how changes in one variable are associated with changes in another.\n",
        "\n",
        "Q13- What does negative correlation mean?\n",
        "\n",
        "A13- A negative correlation means that as one variable increases, the other decreases — they move in opposite directions.\n",
        "\n",
        "Q14- How can you find correlation between variables in Python?\n",
        "\n",
        "A14- To find the correlation between variables in Python, several methods can be employed using libraries like NumPy, SciPy, and Pandas. Here's a breakdown of common approaches: Pandas corr() method: This method, applied to a Pandas DataFrame, calculates the correlation matrix between all pairs of columns. By default, it computes the Pearson correlation coefficient, which measures linear relationships. The correlation values range from -1 to 1, where: 1 indicates a perfect positive correlation. -1 indicates a perfect negative correlation. 0 indicates no linear correlation.\n",
        "\n"
      ],
      "metadata": {
        "id": "fIY1GNkL8Tyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({'A': [1, 2, 3, 4, 5], 'B': [2, 4, 5, 4, 5], 'C': [5, 4, 3, 2, 1]})\n",
        "correlation_matrix = df.corr()\n",
        "print(correlation_matrix)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "A8nyynYN8czx",
        "outputId": "bfae14fd-e8e7-404b-fbca-1f99c36e28e2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          A         B         C\n",
            "A  1.000000  0.774597 -1.000000\n",
            "B  0.774597  1.000000 -0.774597\n",
            "C -1.000000 -0.774597  1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.array([1, 2, 3, 4, 5])\n",
        "y = np.array([2, 4, 5, 4, 5])\n",
        "correlation_matrix = np.corrcoef(x, y)\n",
        "print(correlation_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "uTdKy9lG8hst",
        "outputId": "7df76acf-e01b-4ba6-b17e-e8e724cf9d59"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.         0.77459667]\n",
            " [0.77459667 1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q15- What is causation? Explain difference between correlation and causation with an example.\n",
        "\n",
        "A15- Causation means one event directly causes another to happen, while correlation simply means there is a relationship between two events, but one doesn't necessarily cause the other. For example, if you drop a glass, the glass breaking is caused by the drop. In contrast, ice cream sales and shark attacks might be correlated (both increase in summer), but ice cream sales don't cause shark attacks\n",
        "\n",
        "Q16- What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "\n",
        "A16- An optimizer is an algorithm that adjusts a neural network's parameters to minimize the loss function during training. It's the mechanism that guides the model towards better performance by making small, incremental changes to weights and biases. Types of Optimizers: Here are some common types of optimizers, along with examples:\n",
        "\n",
        "Gradient Descent: Concept: Updates parameters based on the direction of the steepest descent of the loss function. It's a simple but effective method for minimizing a loss function. Example: Imagine a ball rolling down a hill. Gradient descent is like following the steepest path down the hill to reach the bottom (the minimum loss).\n",
        "\n",
        "Stochastic Gradient Descent (SGD): Concept: Updates parameters based on the gradient of the loss function calculated on a single training example or a small batch of examples. It's computationally efficient and can escape local minima more effectively than batch gradient descent. Example: Instead of looking at the entire hill, SGD checks the slope of the hill at each small step (single training example) and moves in that direction.\n",
        "\n",
        "\n",
        "Mini-Batch SGD: Concept: A compromise between batch and stochastic gradient descent, updating parameters using a subset of the training data (a mini-batch). This is a popular and widely used approach because it provides a balance between speed and stability. Example: Like SGD but instead of checking the slope at each point, it checks the slope over a small area (mini-batch) before moving.\n",
        "\n",
        "Momentum: Concept: Introduces a \"momentum\" term to the update rule, accelerating the optimization process and helping to escape local minima. Example: Imagine rolling a ball down a hill. Momentum is like giving the ball a push to continue rolling in the direction of the steepest descent, even if the slope changes slightly.\n",
        "\n",
        "Adam (Adaptive Moment Estimation): Concept: Combines the benefits of both momentum and RMSprop by using both the first moment (mean) and second moment (variance) of the gradients to adjust the learning rate. It's a highly popular optimizer for many deep learning tasks. Example: Adam uses the slope of the hill and also remembers how quickly the slope has been changing to make better decisions about where to move.\n",
        "\n",
        "Nesterov Accelerated Gradient (NAG): Concept: An improved version of momentum that looks ahead before updating the parameters, allowing for more accurate and responsive updates. Example: Like momentum, but instead of just following the slope where you are, NAG looks ahead a little bit to see where the steepest descent might be and adjusts its direction accordingly.\n",
        "\n",
        "Adagrad: Concept: Adapts the learning rate for each parameter based on historical gradients, making it suitable for sparse data. Example: Adagrad is like having different wheels on a car. Each wheel adjusts its size based on how much it's been used to traverse different terrains.\n",
        "\n",
        "RMSprop: Concept: Fixes Adagrad's issue of learning rates shrinking too much by using a moving average of squared gradients. Example: RMSprop uses a \"moving average\" of the \"squashiness\" of the hill to make sure the ball doesn't get stuck in a small dip.\n",
        "\n",
        "Adadelta: Concept: Similar to RMSprop but doesn't require a learning rate hyperparameter. Example: Adadelta adapts the learning rate for each parameter based on the magnitude of the gradients.\n",
        "Q17- What is sklearn.linear_model ?\n",
        "\n",
        "\n",
        "A17- sklearn.linear_model is a module in Scikit-learn (a popular Python machine learning library) that contains linear models for regression and classification tasks.\n",
        "\n",
        "Q18- What does model.fit() do? What arguments must be given?\n",
        "\n",
        "A18- The model.fit() function in machine learning frameworks like TensorFlow and Keras is used to train a model on a given dataset. It adjusts the model's internal parameters, such as weights and biases, to minimize the loss function and improve its performance. The process involves iterating over the dataset in batches for a specified number of epochs, with the model updating its parameters after each batch.\n",
        "\n",
        "Q19- What does model.predict() do? What arguments must be given?\n",
        "\n",
        "A19- The model.predict() function, in the context of machine learning models like those trained in Keras or scikit-learn, is used to make predictions on new, unseen data using a trained model. It essentially passes the input data through the trained model and returns the predicted output, such as class labels or regression values.\n",
        "\n",
        "Q20- What are continuous and categorical variables?\n",
        "\n",
        "A20- In statistics, continuous variables represent numerical values that can take any value within a specified range, while categorical variables represent non-numerical data grouped into categories or groups\n",
        "\n",
        "Q21- What is feature scaling? How does it help in Machine Learning?\n",
        "\n",
        "A21- Feature Scaling: Boost Accuracy and Model PerformanceFeature scaling, also known as normalization or standardization, is a data preprocessing technique that transforms the values of numerical features to a common scale or range. This process is crucial in machine learning because different features in a dataset can have vastly different ranges, which can lead to models being disproportionately influenced by features with larger values. By scaling features, we ensure that all features contribute equally to the model's learning proces\n",
        "\n",
        "Q22- How do we perform scaling in Python?\n",
        "\n",
        "A22- Scaling in Python, particularly within the realm of data preprocessing for machine learning, involves transforming numerical features to a similar range. This prevents features with larger values from dominating those with smaller values, ensuring fair contributions during model training. Several methods exist, each with its own characteristics and use cases.\n",
        "\n",
        "Q23- What is sklearn.preprocessing?\n",
        "\n",
        "A23- sklearn.preprocessing is a module in the scikit-learn library in Python that provides functions and classes to transform raw data into a format suitable for machine learning models. It encompasses various techniques for feature scaling, normalization, encoding categorical variables, and handling missing values. Data preprocessing is a crucial step in the machine learning workflow, as it can significantly impact the performance and accuracy of models.\n",
        "\n",
        "Q24- How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        "A24- In Python, the train_test_split function from the scikit-learn library is commonly used to split data into training and testing sets. This function randomly divides the data into two subsets: one for training the model and the other for evaluating its performance.\n",
        "\n",
        "Q25- Explain data encoding?\n",
        "\n",
        "A25- Understanding Different Types of Encoding and Decoding in ...Data encoding is the process of converting data from one format to another, often to make it more suitable for storage, transmission, or processing"
      ],
      "metadata": {
        "id": "Akznr1nG8pIN"
      }
    }
  ]
}